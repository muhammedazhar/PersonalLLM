{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../Docs/requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2\n",
      "Is Apple MPS (Metal Performance Shader) built? True\n",
      "Is Apple MPS available? True\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "print(f\"Is Apple MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "print(f\"Is Apple MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Set the device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS is available, if yes, use MPS, else check for CUDA\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483074\n",
      "['\\n', ' ', '!', '$', '%', '&', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '£', '©', '°', '·', '×', 'à', 'á', 'ï', 'ó', 'ö', '÷', 'Ā', 'Č', '–', '—', '‘', '’', '“', '”', '…', '∨', '─', '○', '●']\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "with open('../Datasets/Computers-the_machines_we_think_with.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(len(text))\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "print(len(chars))\n",
    "vocabulary_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        29, 41, 39, 42, 47, 46, 31, 44, 45, 99, 46, 34, 31,  1, 39, 27, 29, 34,\n",
      "        35, 40, 31, 45,  1, 49, 31,  1, 46, 34, 35, 40, 37,  1, 49, 35, 46, 34,\n",
      "         0,  0,  0,  0,  0,  0,  0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8*len (data))\n",
    "train_data= data[:n]\n",
    "val_data= data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([1]) target is tensor(1)\n",
      "when input is tensor([1, 1]) target is tensor(1)\n",
      "when input is tensor([1, 1, 1]) target is tensor(1)\n",
      "when input is tensor([1, 1, 1, 1]) target is tensor(1)\n",
      "when input is tensor([1, 1, 1, 1, 1]) target is tensor(1)\n",
      "when input is tensor([1, 1, 1, 1, 1, 1]) target is tensor(1)\n",
      "when input is tensor([1, 1, 1, 1, 1, 1, 1]) target is tensor(1)\n",
      "when input is tensor([1, 1, 1, 1, 1, 1, 1, 1]) target is tensor(1)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range (block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('when input is', context, 'target is', target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PersonalLLM(MPS)",
   "language": "python",
   "name": "personalllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
